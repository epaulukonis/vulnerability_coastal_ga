
library(AICcmodavg)
library(SDMTools)
library(ROCR)
library(raster)
library(rgdal)
library(lme4)
library(corrplot)
library(ggplot2)
library(dismo)
library(caret)
library(hier.part)
library(ltm)

#new eval with TSS/sens/spec/cutoff that optimizes TSS (basically same as max sens/spec)

###AOC
#setwd("E:\\FinalModels\\Predictors\\AOC")
setwd("C:\\Users\\eliza\\Documents\\UGA\\Thesis\\NicheModels_2019\\EvalEdits")
df<-read.csv('AOCfin.csv')

lr04<- glm(PA ~ pland_bh100 + I(pland_bh100^2) + ed_msh1km + I(ed_msh1km^2) + urb1kmf + ow1kmf  + I(ow1kmf^2), family=binomial(link="logit"), df, na.action=na.pass)

#each coef represents the change in the log odds of being 1/0 for a unit increase in that predictor while holding all other predictors constant
#the exp of each coef

## Top model estimates, odds ratios, and 95% CI
coef.results<- cbind(coef(lr04),confint(lr04),OR = exp(coef(lr04)))
coef.results <- cbind(coef.results, exp(confint(lr04)), coef(summary(lr04))[,4])
coef.results <- round(coef.results,3)
colnames(coef.results)[1] <- "Estimate"
colnames(coef.results)[7] <- "p value"
#OR represents the odds that an outcome will occur given a particular predictor, compared to the odds of the outcome occuring without the predictor

write.csv(coef.results,'AOC_coefresultsn.csv')


df$fitted<-fitted(lr04)
( lrAUC<- auc(df$PA, df$fitted) )
#biserial correlation
bis<-cor.test(df$fitted,df$PA)
biserial<-biserial.cor(df$fitted,df$PA)*-1
biserial
#really just a measure of how well the fitted data correlates with the original PA data
pred <- prediction(df$fitted, df$PA)
perf <- performance(pred, measure="tpr", x.measure="fpr")
plot(perf, col=rainbow(10),main='ROC Curve, AOC')


opt.cut = function(perf, pred){
  cut.ind = mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}

perf <- data.frame(AUC=NA,sens=NA,spec=NA,TSS=NA,cutoff=NA)
pred <- predict(lr04, df, type = "response")
pred.lab <- as.data.frame(cbind(pred,df$PA))
colnames(pred.lab) <- c("predictions","labels")
pred.lab1 <- prediction(pred.lab$predictions,pred.lab$labels) 
perf.auc <- performance(prediction(pred, df$PA),"auc")
perf.roc <- performance(pred.lab1, measure = "tpr", x.measure = "fpr")
perf.sens <- performance(prediction(pred,df$PA), "sens","spec")
cutoff <- opt.cut(perf.roc, pred.lab1)
perf$AUC <- perf.auc@y.values[[1]] # AUC
perf$sens <- cutoff["sensitivity",] # sensitivity
perf$spec <- cutoff["specificity",] # specificity
perf$cutoff <- cutoff["cutoff",] # optimal cutoff
perf$TSS <- perf$sens + perf$spec - 1

write.csv(perf,'AOC_evalresults.csv')



###BASP
setwd("C:\\Users\\eliza\\Documents\\UGA\\Thesis\\NicheModels_2019\\EvalEdits")
df<-read.csv('BASP.csv')

lr04<- glm(PA ~ plandpine800 + I(plandpine800^2) + fire800 + I(fire800^2)  + herbht800 + shrbht800 + can100 + I(can100^2), family=binomial(link="logit"), df, na.action=na.pass)

#each coef represents the change in the log odds of being 1/0 for a unit increase in that predictor while holding all other predictors constant
#the exp of each coef

## Top model estimates, odds ratios, and 95% CI
coef.results<- cbind(coef(lr04),confint(lr04),OR = exp(coef(lr04)))
coef.results <- cbind(coef.results, exp(confint(lr04)), coef(summary(lr04))[,4])
coef.results <- round(coef.results,3)
colnames(coef.results)[1] <- "Estimate"
colnames(coef.results)[7] <- "p value"
#OR represents the odds that an outcome will occur given a particular predictor, compared to the odds of the outcome occuring without the predictor

write.csv(coef.results,'BASP_coefresultsn.csv')

df$fitted<-fitted(lr04)
( lrAUC<- auc(df$PA, df$fitted) )
#biserial correlation
bis<-cor.test(df$fitted,df$PA)
biserial<-biserial.cor(df$fitted,df$PA)*-1
biserial
#really just a measure of how well the fitted data correlates with the original PA data
pred <- prediction(df$fitted, df$PA)
perf <- performance(pred, measure="tpr", x.measure="fpr")
plot(perf, col=rainbow(10),main='ROC Curve, AOC')


opt.cut = function(perf, pred){
  cut.ind = mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}

perf <- data.frame(AUC=NA,sens=NA,spec=NA,TSS=NA,cutoff=NA)
pred <- predict(lr04, df, type = "response")
pred.lab <- as.data.frame(cbind(pred,df$PA))
colnames(pred.lab) <- c("predictions","labels")
pred.lab1 <- prediction(pred.lab$predictions,pred.lab$labels) 
perf.auc <- performance(prediction(pred, df$PA),"auc")
perf.roc <- performance(pred.lab1, measure = "tpr", x.measure = "fpr")
perf.sens <- performance(prediction(pred,df$PA), "sens","spec")
cutoff <- opt.cut(perf.roc, pred.lab1)
perf$AUC <- perf.auc@y.values[[1]] # AUC
perf$sens <- cutoff["sensitivity",] # sensitivity
perf$spec <- cutoff["specificity",] # specificity
perf$cutoff <- cutoff["cutoff",] # optimal cutoff
perf$TSS <- perf$sens + perf$spec - 1

write.csv(perf,'BASP_evalresults.csv')

###DT
setwd("C:\\Users\\eliza\\Documents\\UGA\\Thesis\\NicheModels_2019\\EvalEdits")
df<-read.csv('DT.csv')
lr01<- glm(PA ~ marsh500 + landco_800 + I(landco_800^2) + urb_800 + I(urb_800^2) + elev500, family=binomial(link="logit"), df, na.action=na.pass)
summary(lr01)
#each coef represents the change in the log odds of being 1/0 for a unit increase in that predictor while holding all other predictors constant
#the exp of each coef

## Top model estimates, odds ratios, and 95% CI
coef.results<- cbind(coef(lr01),confint(lr01),OR = exp(coef(lr01)))
coef.results <- cbind(coef.results, exp(confint(lr01)), coef(summary(lr01))[,4])
colnames(coef.results)[1] <- "Estimate"
colnames(coef.results)[7] <- "p value"
#OR represents the odds that an outcome will occur given a particular predictor, compared to the odds of the outcome occuring without the predictor
write.csv(coef.results,'DT_coefresults.csv')

df$fitted<-fitted(lr01)
( lrAUC<- auc(df$PA, df$fitted) )
#biserial correlation
bis<-cor.test(df$fitted,df$PA)
biserial<-biserial.cor(df$fitted,df$PA)*-1
biserial
#really just a measure of how well the fitted data correlates with the original PA data
pred <- prediction(df$fitted, df$PA)
#look at ROC curve
perf <- performance(pred, measure="tpr", x.measure="fpr")
plot(perf, col=rainbow(10),main='ROC Curve, AOC')


opt.cut = function(perf, pred){
  cut.ind = mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}

perf <- data.frame(AUC=NA,sens=NA,spec=NA,TSS=NA,cutoff=NA)
pred <- predict(lr01, df, type = "response")
pred.lab <- as.data.frame(cbind(pred,df$PA))
colnames(pred.lab) <- c("predictions","labels")
pred.lab1 <- prediction(pred.lab$predictions,pred.lab$labels) 
perf.auc <- performance(prediction(pred, df$PA),"auc")
perf.roc <- performance(pred.lab1, measure = "tpr", x.measure = "fpr")
perf.sens <- performance(prediction(pred,df$PA), "sens","spec")
cutoff <- opt.cut(perf.roc, pred.lab1)
perf$AUC <- perf.auc@y.values[[1]] # AUC
perf$sens <- cutoff["sensitivity",] # sensitivity
perf$spec <- cutoff["specificity",] # specificity
perf$cutoff <- cutoff["cutoff",] # optimal cutoff
perf$TSS <- perf$sens + perf$spec - 1

write.csv(perf,'DT_evalresults.csv')

###EDR
setwd("C:\\Users\\eliza\\Documents\\UGA\\Thesis\\NicheModels_2019\\EvalEdits")
df<-read.csv('EDR.csv')

lr01<- glm(PA ~ can900 + dran250 + fire900 + landco250  + tpi_raw +urb_250 +  precip_raw + I(precip_raw^2), family=binomial(link="logit"), df, na.action=na.pass) 
summary(lr01)
#each coef represents the change in the log odds of being 1/0 for a unit increase in that predictor while holding all other predictors constant
#the exp of each coef

## Top model estimates, odds ratios, and 95% CI
coef.results<- cbind(coef(lr01),confint(lr01),OR = exp(coef(lr01)))
coef.results <- cbind(coef.results, exp(confint(lr01)), coef(summary(lr01))[,4])
colnames(coef.results)[1] <- "Estimate"
colnames(coef.results)[7] <- "p value"
#OR represents the odds that an outcome will occur given a particular predictor, compared to the odds of the outcome occuring without the predictor
write.csv(coef.results,'EDR_coefresults.csv')

df$fitted<-fitted(lr01)
( lrAUC<- auc(df$PA, df$fitted) )
#biserial correlation
bis<-cor.test(df$fitted,df$PA)
biserial<-biserial.cor(df$fitted,df$PA)*-1
biserial
#really just a measure of how well the fitted data correlates with the original PA data
pred <- prediction(df$fitted, df$PA)
#look at ROC curve
perf <- performance(pred, measure="tpr", x.measure="fpr")
plot(perf, col=rainbow(10),main='ROC Curve, AOC')


opt.cut = function(perf, pred){
  cut.ind = mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}

perf <- data.frame(AUC=NA,sens=NA,spec=NA,TSS=NA,cutoff=NA)
pred <- predict(lr01, df, type = "response")
pred.lab <- as.data.frame(cbind(pred,df$PA))
colnames(pred.lab) <- c("predictions","labels")
pred.lab1 <- prediction(pred.lab$predictions,pred.lab$labels) 
perf.auc <- performance(prediction(pred, df$PA),"auc")
perf.roc <- performance(pred.lab1, measure = "tpr", x.measure = "fpr")
perf.sens <- performance(prediction(pred,df$PA), "sens","spec")
cutoff <- opt.cut(perf.roc, pred.lab1)
perf$AUC <- perf.auc@y.values[[1]] # AUC
perf$sens <- cutoff["sensitivity",] # sensitivity
perf$spec <- cutoff["specificity",] # specificity
perf$cutoff <- cutoff["cutoff",] # optimal cutoff
perf$TSS <- perf$sens + perf$spec - 1

write.csv(perf,'EDR_evalresults.csv')

###EIS
setwd("C:\\Users\\eliza\\Documents\\UGA\\Thesis\\NicheModels_2019\\EvalEdits")
df<-read.csv('EIS.csv')

lr01<- glm(PA ~ rip_900 + can900 + pine900 + dran250  + landco250  + precip_raw + I(precip_raw^2) +urb_250  +hist900, family=binomial(link="logit"), df, na.action=na.pass) 
summary(lr01)
#each coef represents the change in the log odds of being 1/0 for a unit increase in that predictor while holding all other predictors constant
#the exp of each coef

## Top model estimates, odds ratios, and 95% CI
coef.results<- cbind(coef(lr01),confint(lr01),OR = exp(coef(lr01)))
coef.results <- cbind(coef.results, exp(confint(lr01)), coef(summary(lr01))[,4])
colnames(coef.results)[1] <- "Estimate"
colnames(coef.results)[7] <- "p value"
#OR represents the odds that an outcome will occur given a particular predictor, compared to the odds of the outcome occuring without the predictor
write.csv(coef.results,'EIS_coefresults.csv')

df$fitted<-fitted(lr01)
( lrAUC<- auc(df$PA, df$fitted) )
#biserial correlation
bis<-cor.test(df$fitted,df$PA)
biserial<-biserial.cor(df$fitted,df$PA)*-1
biserial
#really just a measure of how well the fitted data correlates with the original PA data
pred <- prediction(df$fitted, df$PA)
#look at ROC curve
perf <- performance(pred, measure="tpr", x.measure="fpr")
plot(perf, col=rainbow(10),main='ROC Curve, AOC')


opt.cut = function(perf, pred){
  cut.ind = mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}

perf <- data.frame(AUC=NA,sens=NA,spec=NA,TSS=NA,cutoff=NA)
pred <- predict(lr01, df, type = "response")
pred.lab <- as.data.frame(cbind(pred,df$PA))
colnames(pred.lab) <- c("predictions","labels")
pred.lab1 <- prediction(pred.lab$predictions,pred.lab$labels) 
perf.auc <- performance(prediction(pred, df$PA),"auc")
perf.roc <- performance(pred.lab1, measure = "tpr", x.measure = "fpr")
perf.sens <- performance(prediction(pred,df$PA), "sens","spec")
cutoff <- opt.cut(perf.roc, pred.lab1)
perf$AUC <- perf.auc@y.values[[1]] # AUC
perf$sens <- cutoff["sensitivity",] # sensitivity
perf$spec <- cutoff["specificity",] # specificity
perf$cutoff <- cutoff["cutoff",] # optimal cutoff
perf$TSS <- perf$sens + perf$spec - 1

write.csv(perf,'EIS_evalresults.csv')


###PB
setwd("C:\\Users\\eliza\\Documents\\UGA\\Thesis\\NicheModels_2019\\EvalEdits")
df<-read.csv('PBfinedits.csv')

lr01<- glm(PA ~ patmshb700 + shrbht700 + edmar700  + can700 + I(can700^2), family=binomial(link="logit"), df, na.action=na.pass) 
summary(lr01)
#each coef represents the change in the log odds of being 1/0 for a unit increase in that predictor while holding all other predictors constant
#the exp of each coef

## Top model estimates, odds ratios, and 95% CI
coef.results<- cbind(coef(lr01),confint(lr01),OR = exp(coef(lr01)))
coef.results <- cbind(coef.results, exp(confint(lr01)), coef(summary(lr01))[,4])
colnames(coef.results)[1] <- "Estimate"
colnames(coef.results)[7] <- "p value"
#OR represents the odds that an outcome will occur given a particular predictor, compared to the odds of the outcome occuring without the predictor
write.csv(coef.results,'PB_coefresults.csv')

df$fitted<-fitted(lr01)
( lrAUC<- auc(df$PA, df$fitted) )
#biserial correlation
bis<-cor.test(df$fitted,df$PA)
biserial<-biserial.cor(df$fitted,df$PA)*-1
biserial
#really just a measure of how well the fitted data correlates with the original PA data
pred <- prediction(df$fitted, df$PA)
#look at ROC curve
perf <- performance(pred, measure="tpr", x.measure="fpr")
plot(perf, col=rainbow(10),main='ROC Curve, AOC')


opt.cut = function(perf, pred){
  cut.ind = mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}

perf <- data.frame(AUC=NA,sens=NA,spec=NA,TSS=NA,cutoff=NA)
pred <- predict(lr01, df, type = "response")
pred.lab <- as.data.frame(cbind(pred,df$PA))
colnames(pred.lab) <- c("predictions","labels")
pred.lab1 <- prediction(pred.lab$predictions,pred.lab$labels) 
perf.auc <- performance(prediction(pred, df$PA),"auc")
perf.roc <- performance(pred.lab1, measure = "tpr", x.measure = "fpr")
perf.sens <- performance(prediction(pred,df$PA), "sens","spec")
cutoff <- opt.cut(perf.roc, pred.lab1)
perf$AUC <- perf.auc@y.values[[1]] # AUC
perf$sens <- cutoff["sensitivity",] # sensitivity
perf$spec <- cutoff["specificity",] # specificity
perf$cutoff <- cutoff["cutoff",] # optimal cutoff
perf$TSS <- perf$sens + perf$spec - 1

write.csv(perf,'PB_evalresults.csv')

###RCW
df<-read.csv('RCW.csv')

lr01<- glm(PA ~  plandpine800 + I(plandpine800^2) + fire800 + I(fire800^2)  + herbht800 + shrbht800 + can800, family=binomial(link="logit"), df, na.action=na.pass) 
summary(lr01)

## Top model estimates, odds ratios, and 95% CI
coef.results<- cbind(coef(lr01),confint(lr01),OR = exp(coef(lr01)))
coef.results <- cbind(coef.results, exp(confint(lr01)), coef(summary(lr01))[,4])
colnames(coef.results)[1] <- "Estimate"
colnames(coef.results)[7] <- "p value"
#OR represents the odds that an outcome will occur given a particular predictor, compared to the odds of the outcome occuring without the predictor
write.csv(coef.results,'RCW_coefresults.csv')

df$fitted<-fitted(lr01)
( lrAUC<- auc(df$PA, df$fitted) )
#biserial correlation
bis<-cor.test(df$fitted,df$PA)
biserial<-biserial.cor(df$fitted,df$PA)*-1
biserial
#really just a measure of how well the fitted data correlates with the original PA data
pred <- prediction(df$fitted, df$PA)
#look at ROC curve
perf <- performance(pred, measure="tpr", x.measure="fpr")
plot(perf, col=rainbow(10),main='ROC Curve, AOC')


opt.cut = function(perf, pred){
  cut.ind = mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}

perf <- data.frame(AUC=NA,sens=NA,spec=NA,TSS=NA,cutoff=NA)
pred <- predict(lr01, df, type = "response")
pred.lab <- as.data.frame(cbind(pred,df$PA))
colnames(pred.lab) <- c("predictions","labels")
pred.lab1 <- prediction(pred.lab$predictions,pred.lab$labels) 
perf.auc <- performance(prediction(pred, df$PA),"auc")
perf.roc <- performance(pred.lab1, measure = "tpr", x.measure = "fpr")
perf.sens <- performance(prediction(pred,df$PA), "sens","spec")
cutoff <- opt.cut(perf.roc, pred.lab1)
perf$AUC <- perf.auc@y.values[[1]] # AUC
perf$sens <- cutoff["sensitivity",] # sensitivity
perf$spec <- cutoff["specificity",] # specificity
perf$cutoff <- cutoff["cutoff",] # optimal cutoff
perf$TSS <- perf$sens + perf$spec - 1

write.csv(perf,'RCW_evalresults.csv')


###SSS
# df<-read.csv('SSS.csv')
# 
# lr01<- glm(PA ~  ed_msh200  + elev200f + brack200 +urb_200 +for_200, family=binomial(link="logit"), df, na.action=na.pass) 
# summary(lr01)
# 
# ## Top model estimates, odds ratios, and 95% CI
# coef.results<- cbind(coef(lr01),confint(lr01),OR = exp(coef(lr01)))
# coef.results <- cbind(coef.results, exp(confint(lr01)), coef(summary(lr01))[,4])
# colnames(coef.results)[1] <- "Estimate"
# colnames(coef.results)[7] <- "p value"
# #OR represents the odds that an outcome will occur given a particular predictor, compared to the odds of the outcome occuring without the predictor
# write.csv(coef.results,'SSS_coefresults.csv')
# 
# df$fitted<-fitted(lr01)
# ( lrAUC<- auc(df$PA, df$fitted) )
# #biserial correlation
# bis<-cor.test(df$fitted,df$PA)
# biserial<-biserial.cor(df$fitted,df$PA)*-1
# biserial
# #really just a measure of how well the fitted data correlates with the original PA data
# pred <- prediction(df$fitted, df$PA)
# #look at ROC curve
# perf <- performance(pred, measure="tpr", x.measure="fpr")
# plot(perf, col=rainbow(10),main='ROC Curve, AOC')
# 
# 
# opt.cut = function(perf, pred){
#   cut.ind = mapply(FUN=function(x, y, p){
#     d = (x - 0)^2 + (y-1)^2
#     ind = which(d == min(d))
#     c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
#       cutoff = p[[ind]])
#   }, perf@x.values, perf@y.values, pred@cutoffs)
# }
# 
# perf <- data.frame(AUC=NA,sens=NA,spec=NA,TSS=NA,cutoff=NA)
# pred <- predict(lr01, df, type = "response")
# pred.lab <- as.data.frame(cbind(pred,df$PA))
# colnames(pred.lab) <- c("predictions","labels")
# pred.lab1 <- prediction(pred.lab$predictions,pred.lab$labels) 
# perf.auc <- performance(prediction(pred, df$PA),"auc")
# perf.roc <- performance(pred.lab1, measure = "tpr", x.measure = "fpr")
# perf.sens <- performance(prediction(pred,df$PA), "sens","spec")
# cutoff <- opt.cut(perf.roc, pred.lab1)
# perf$AUC <- perf.auc@y.values[[1]] # AUC
# perf$sens <- cutoff["sensitivity",] # sensitivity
# perf$spec <- cutoff["specificity",] # specificity
# perf$cutoff <- cutoff["cutoff",] # optimal cutoff
# perf$TSS <- perf$sens + perf$spec - 1
# 
# write.csv(perf,'SSS_evalresults.csv')





#WP
#setwd("E:\\FinalModels\\Predictors\\WS")
setwd("C:\\Users\\eliza\Documents\\UGA\\Thesis\\NicheModels_2019\\EvalEdits")
df<-read.csv('WPnewedits.csv')

lr01<-  glm(PA ~ pland_bh100f  + ed_msh100 + landco100  + urb1kmf + elev1kmf, family=binomial(link="logit"), df, na.action=na.pass) 

#each coef represents the change in the log odds of being 1/0 for a unit increase in that predictor while holding all other predictors constant
#the exp of each coef

## Top model estimates, odds ratios, and 95% CI
coef.results<- cbind(coef(lr01),confint(lr01),OR = exp(coef(lr01)))
coef.results <- cbind(coef.results, exp(confint(lr01)), coef(summary(lr01))[,4])
colnames(coef.results)[1] <- "Estimate"
colnames(coef.results)[7] <- "p value"
#OR represents the odds that an outcome will occur given a particular predictor, compared to the odds of the outcome occuring without the predictor
write.csv(coef.results,'WP_coefresults.csv')

df$fitted<-fitted(lr01)
( lrAUC<- auc(df$PA, df$fitted) )
#biserial correlation
bis<-cor.test(df$fitted,df$PA)
biserial<-biserial.cor(df$fitted,df$PA)*-1
biserial
#really just a measure of how well the fitted data correlates with the original PA data
pred <- prediction(df$fitted, df$PA)
#look at ROC curve
perf <- performance(pred, measure="tpr", x.measure="fpr")
plot(perf, col=rainbow(10),main='ROC Curve, AOC')


opt.cut = function(perf, pred){
  cut.ind = mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}

perf <- data.frame(AUC=NA,sens=NA,spec=NA,TSS=NA,cutoff=NA)
pred <- predict(lr01, df, type = "response")
pred.lab <- as.data.frame(cbind(pred,df$PA))
colnames(pred.lab) <- c("predictions","labels")
pred.lab1 <- prediction(pred.lab$predictions,pred.lab$labels) 
perf.auc <- performance(prediction(pred, df$PA),"auc")
perf.roc <- performance(pred.lab1, measure = "tpr", x.measure = "fpr")
perf.sens <- performance(prediction(pred,df$PA), "sens","spec")
cutoff <- opt.cut(perf.roc, pred.lab1)
perf$AUC <- perf.auc@y.values[[1]] # AUC
perf$sens <- cutoff["sensitivity",] # sensitivity
perf$spec <- cutoff["specificity",] # specificity
perf$cutoff <- cutoff["cutoff",] # optimal cutoff
perf$TSS <- perf$sens + perf$spec - 1

write.csv(perf,'WP_evalresults.csv')



#WS
#setwd("E:\\FinalModels\\Predictors\\WS")
setwd("C:\\Users\\eliza\Documents\\UGA\\Thesis\\NicheModels_2019\\EvalEdits")
df<-read.csv('WSfinfin1010.csv')
lr01 <- glm(PA ~ nwi2000 + wat2000 +  I(wat2000^2) + nwifwd_2000 + nhd_2000 + landco2000 +  I(landco2000^2) + can2000, family=binomial(link="logit"), df, na.action=na.pass) 
summary(lr01)
#each coef represents the change in the log odds of being 1/0 for a unit increase in that predictor while holding all other predictors constant
#the exp of each coef

## Top model estimates, odds ratios, and 95% CI
coef.results<- cbind(coef(lr01),confint(lr01),OR = exp(coef(lr01)))
coef.results <- cbind(coef.results, exp(confint(lr01)), coef(summary(lr01))[,4])
colnames(coef.results)[1] <- "Estimate"
colnames(coef.results)[7] <- "p value"
#OR represents the odds that an outcome will occur given a particular predictor, compared to the odds of the outcome occuring without the predictor
write.csv(coef.results,'WS_coefresults.csv')

df$fitted<-fitted(lr01)
( lrAUC<- auc(df$PA, df$fitted) )
#biserial correlation
bis<-cor.test(df$fitted,df$PA)
biserial<-biserial.cor(df$fitted,df$PA)*-1
biserial
#really just a measure of how well the fitted data correlates with the original PA data
pred <- prediction(df$fitted, df$PA)
#look at ROC curve
perf <- performance(pred, measure="tpr", x.measure="fpr")
plot(perf, col=rainbow(10),main='ROC Curve, AOC')


opt.cut = function(perf, pred){
  cut.ind = mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}

perf <- data.frame(AUC=NA,sens=NA,spec=NA,TSS=NA,cutoff=NA)
pred <- predict(lr01, df, type = "response")
pred.lab <- as.data.frame(cbind(pred,df$PA))
colnames(pred.lab) <- c("predictions","labels")
pred.lab1 <- prediction(pred.lab$predictions,pred.lab$labels) 
perf.auc <- performance(prediction(pred, df$PA),"auc")
perf.roc <- performance(pred.lab1, measure = "tpr", x.measure = "fpr")
perf.sens <- performance(prediction(pred,df$PA), "sens","spec")
cutoff <- opt.cut(perf.roc, pred.lab1)
perf$AUC <- perf.auc@y.values[[1]] # AUC
perf$sens <- cutoff["sensitivity",] # sensitivity
perf$spec <- cutoff["specificity",] # specificity
perf$cutoff <- cutoff["cutoff",] # optimal cutoff
perf$TSS <- perf$sens + perf$spec - 1

write.csv(perf,'WS_evalresults.csv')

